This codebase accompanies our preprint: [Reinforcement Learning for Flow-Matching Policies](https://arxiv.org/abs/2507.15073).

## Installation
This codebase requires python 3.12. Install it via [pyenv](https://github.com/pyenv/pyenv) if not already present and then run:

```bash
mkvirtualenv flowmatchingrl -p ~/.pyenv/versions/3.12.10/bin/python3.12
pip install -e .
```


## Execution
Our experiments and data artifacts are orchestrated through weights and biases. First log in to a `wandb` account. Training data can then be generated by running:

```bash
python src/envs/unicycle/data.py
```

This should take approximately 5-10 minutes. To run the experiments, first create the sweeps via:

```bash
wandb sweep --project "flowmatchingrl" ./scripts/rwfm_sweep.yaml
wandb sweep --project "flowmatchingrl" ./scripts/grpo_sweep.yaml
```

And execute the corresponding `wandb agent` commands included in the terminal output. We note that there is no dedicated `ilfm_sweep.yaml` as ILFM is a special case of RWFM.

Experimental plots can be reproduced using `plotting/plot.py`, passing in your `wandb` sweep ids as arguments.
